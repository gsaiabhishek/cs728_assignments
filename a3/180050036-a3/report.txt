Question 1:

1. 
Used webqsp_elq dataset for training and evaluation. ELQ paper doesn't assume mentions span
and hence derives a probability on that. But they also F1 score for enitity linking only(and not mention detection).
We will be comapring to that scores. Since our method assumes mention span, I used gold mention 
spans from the webqsp_elq dataset and only linked them to entities using the approach
described in question. The F1 observed by ELQ is 90.2. While our model gives an F1 score of 91.5.

2. 
(bonus question) Among the 30 citations to ELQ in google scholar, this is what I found:
    1. https://arxiv.org/pdf/2012.14610.pdf explores the idea in question. It uses ELQ to retrieve
        some entities before combining them into short passages and feeding to FiD reader.
    2. https://arxiv.org/pdf/2109.08678.pdf and https://arxiv.org/pdf/2104.08762.pdf use a similar idea. 
        Former uses a generation model to convert the candidate logical forms for a given question into a single logical form.
        Latter employs a similar scheme.





Question 2:

1.
The model I used is a bert-base-cased from transformers along with a FFN(with relu activation) which takes ouptput of
[CLS] token and gives #labels(6 for coarse, 50 for fine) number of outputs which are then softmaxed to 
predict the answer type for a question. A dropout of 0.3 is used between bert and FFN. 

Best train and test accuracies observed are 98.4% and 97% respectively when the model is ran for 
10 epochs and trained on coarse labels. 
{This is higher than the accuracy obtained with lstm models of trec dataset(which I have worked on before) and this 
could be attributed to the extensive pre-training and non-directional contextual information usage of bert.}

While the best train and test accuracies 85.9% and 75.8% respectively when the model is ran for 15 epochs and 
trained on fine labels. If 25 epochs are used, accuracies are 92.6% and 84.2% respectively. This indicates that 
increase in granularity of the output labels cause the decrease in accuracy of prediction of answer type of a question
and it gets more difficult to achieve accuracies obtained with coarse labels(fine labels can't achieve the accuracy obtained
by coarse labels(in 10 epochs) even with 25 epochs).
